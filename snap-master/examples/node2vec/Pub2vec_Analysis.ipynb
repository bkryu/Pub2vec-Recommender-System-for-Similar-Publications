{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d32 = np.loadtxt('./emb/d32combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d32_ids = d32[:,0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('inv_id_map.json','r')\n",
    "for line in json_file:\n",
    "    inv_id_map = json.loads(line)\n",
    "\n",
    "json_file = open('id_map.json','r')    \n",
    "for line in json_file:\n",
    "    id_map = json.loads(line)\n",
    "    \n",
    "json_file = open('citation_map.json','r')    \n",
    "for line in json_file:\n",
    "    citation_map = json.loads(line)\n",
    "    \n",
    "json_file = open('fos_map.json','r')    \n",
    "for line in json_file:\n",
    "    fos_map = json.loads(line)    \n",
    "    \n",
    "json_file = open('name_map.json','r')    \n",
    "for line in json_file:\n",
    "    name_map = json.loads(line)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "def fos_distance(id_1, id_2):\n",
    "    id_1str = str(id_1)\n",
    "    id_2str = str(id_2)\n",
    "    \n",
    "    if not(id_1str in fos_map):\n",
    "        return 100.0\n",
    "    if not(id_2str in fos_map):\n",
    "        return 100.0\n",
    "    id_1_fos = fos_map[id_1str]\n",
    "    id_2_fos = fos_map[id_2str]\n",
    "    \n",
    "    combined_fos = set()\n",
    "    \n",
    "    for key in id_1_fos.keys():\n",
    "        combined_fos.add(key)\n",
    "    for key in id_2_fos.keys():\n",
    "        combined_fos.add(key)\n",
    "    \n",
    "    distance = 0.0    \n",
    "    for key in combined_fos:\n",
    "\n",
    "        w1 = key in id_1_fos and id_1_fos[key] or 0.0\n",
    "        w2 = key in id_2_fos and id_2_fos[key] or 0.0\n",
    "        distance += np.power(w1 - w2,2)\n",
    "\n",
    "    return distance   \n",
    "\n",
    "def pub2vec_distance(id_1, id_2):\n",
    "    idx_1 = np.nonzero(d32_ids==id_map[str(id_1)])[0][0]\n",
    "    idx_2 = np.nonzero(d32_ids==id_map[str(id_2)])[0][0]\n",
    "    \n",
    "    feature_1 = np.copy(d32[idx_1, 1:])\n",
    "    feature_2 = np.copy(d32[idx_2, 1:])\n",
    "    \n",
    "    return np.linalg.norm(feature_1 - feature_2)\n",
    "\n",
    "def pub2vec_closest_k(node_id, k):\n",
    "    idx = np.nonzero(d32_ids==id_map[str(node_id)])[0][0]\n",
    "    featurevec = np.copy(d32[idx, 1:])\n",
    "    \n",
    "    l2_diff = np.power(d32[:, 1:] - featurevec, 2)\n",
    "    l2_diff = np.sum(l2_diff, axis=1)\n",
    "    \n",
    "    sorted_indices = np.argsort(l2_diff)\n",
    "    \n",
    "    \n",
    "    return d32_ids[sorted_indices[0:k]]\n",
    "    \n",
    "def return_distances(node_idx, k):\n",
    "    top_k = pub2vec_closest_k(node_idx, k+1)\n",
    "    \n",
    "    fos_vector = []\n",
    "    p2v_vector = []\n",
    "    \n",
    "    for i in range(1, k+1):\n",
    "        fos_vector.append(fos_distance(id_map[str(node_idx)], str(top_k[i])) )\n",
    "        p2v_vector.append(pub2vec_distance(node_idx, inv_id_map[str(top_k[i])]))\n",
    "    return (fos_vector, p2v_vector)        \n",
    "\n",
    "def random_distances(node_idx, k):\n",
    "    \n",
    "    node_ids = random.sample(id_map.keys(),k)\n",
    "    fos_vector = []\n",
    "    p2v_vector = []   \n",
    "    \n",
    "    for i in range(k):\n",
    "        random_node = node_ids[i]\n",
    "#         print(random_node)\n",
    "        fos_d = 0\n",
    "        p2v_d = 0\n",
    "        try:\n",
    "            fos_d = fos_distance(id_map[str(node_idx)], id_map[str(random_node)])\n",
    "            p2v_d = pub2vec_distance(node_idx, random_node)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if not(fos_d == 0 and p2v_d == 0):\n",
    "            fos_vector.append(fos_d)\n",
    "            p2v_vector.append(p2v_d)\n",
    "\n",
    "    return (fos_vector, p2v_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some node IDs for The papers I have used\n",
    "\n",
    "K-means++: 2073459066\n",
    "\n",
    "Googlenet: 2097117768\n",
    "\n",
    "Adagrad: 2146502635\n",
    "\n",
    "node2vec: 2366141641\n",
    "\n",
    "Edge Detection: 2145023731\n",
    "\n",
    "RL: 2107726111\n",
    "\n",
    "Quicksort: 2082357899\n",
    "\n",
    "Latent Dirichelt Allocation: 1880262756\n",
    "\n",
    "A*: 1969483458\n",
    "\n",
    "Spark: 2189465200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2vec: Scalable Feature Learning for Networks\n",
      "LINE: Large-scale Information Network Embedding\n",
      "A High-Performance Semi-Supervised Learning Method for Text Chunking\n",
      "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables\n",
      "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification\n",
      "Zero-Shot Learning Through Cross-Modal Transfer\n",
      "Learning Multilevel Distributed Representations for High-Dimensional Sequences\n",
      "Relational learning via latent social dimensions\n",
      "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification\n",
      "A New Baseline for Image Annotation\n"
     ]
    }
   ],
   "source": [
    "top_k = pub2vec_closest_k(2366141641, 10)\n",
    "for node_id in top_k:\n",
    "    similar_node_id = inv_id_map[str(node_id)]\n",
    "    print(name_map[str(node_id)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-means++: the advantages of careful seeding\n",
      "Clustering of the self-organizing map\n",
      "Integrating constraints and metric learning in semi-supervised clustering\n",
      "Data clustering: 50 years beyond K-means\n",
      "Using the triangle inequality to accelerate k-means\n",
      "Comparing Clusterings by the Variation of Information\n",
      "A stability based method for discovering structure in clustered data\n",
      "Comparing clusterings: an axiomatic view\n",
      "SIMPLIcity: semantics-sensitive integrated matching for picture libraries\n",
      "Consensus Clustering: A Resampling-Based Method for Class Discovery and Visualization of Gene Expression Microarray Data\n"
     ]
    }
   ],
   "source": [
    "top_k = pub2vec_closest_k(2073459066, 10)\n",
    "for node_id in top_k:\n",
    "    similar_node_id = inv_id_map[str(node_id)]\n",
    "    print(name_map[str(node_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: cluster computing with working sets\n",
      "Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing\n",
      "A comparison of approaches to large-scale data analysis\n",
      "Map-reduce-merge: simplified relational data processing on large clusters\n",
      "Evaluating MapReduce for Multi-core and Multiprocessor Systems\n",
      "SCOPE: easy and efficient parallel processing of massive data sets\n",
      "Pig latin: a not-so-foreign language for data processing\n",
      "Hive: a warehousing solution over a map-reduce framework\n",
      "Improving MapReduce performance in heterogeneous environments\n",
      "Dryad: distributed data-parallel programs from sequential building blocks\n"
     ]
    }
   ],
   "source": [
    "top_k = pub2vec_closest_k(2189465200, 10)\n",
    "for node_id in top_k:\n",
    "    similar_node_id = inv_id_map[str(node_id)]\n",
    "    print(name_map[str(node_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going deeper with convolutions\n",
      "Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
      "Caffe: Convolutional Architecture for Fast Feature Embedding\n",
      "ImageNet Classification with Deep Convolutional Neural Networks\n",
      "Dropout: a simple way to prevent neural networks from overfitting\n",
      "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\n",
      "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\n",
      "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
      "Fully convolutional networks for semantic segmentation\n",
      "ImageNet classification with deep convolutional neural networks\n"
     ]
    }
   ],
   "source": [
    "top_k = pub2vec_closest_k(2097117768, 10)\n",
    "for node_id in top_k:\n",
    "    similar_node_id = inv_id_map[str(node_id)]\n",
    "    print(name_map[str(node_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
